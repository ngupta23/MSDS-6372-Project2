Week 1 (9th - 16th)
 - Introduction (send for approval) [Nikhil]
 - [Option 1] Feature Engineering (Basic with only interaction terms) [Max]
 - [Option 2] Export grouping from R (dataset with group variable so it can be used in SAS) [Max]
 
Week 2 (17th - 24th)
 - [Option 1] PCR with Basic Feature Engineering [Max]
              2 way interactions of all variables. PCA is working
              However, feature selection (Forward, Backward, Stepwise running for over 1 week and still not done)
              LASSO and LARS is working.
              Observations: 
              (1) Full Model is giving very good residuals (normal, no skew as before), but it may be overfitting 
              (2) Need to check with Test set if it is really overfitting [Max]
              (3) LARS and LASSO runs, but results (residuals) are not good)
 - [Option 1] Advanced Feature Engineering (more advanced feature engineering with domain expertise) [Nikhil]
              Observations:
              (1) PCA with original variables (no interaction) -- since original variables were not correlated, we saw slow rise for the scree plot (i.e. many PCA's were needed to explain 80% variability)
              (2) With 2 way interactions (with original variables), ~3800 PCA's explain 80% of variance -- Full model with 3800 PCA's gives good results on train set (see above)
              (3) With Domain specific feature engineered terms, # PCA to explain 90% variability is only 164 (this includes 2 way interactions between x variables only - no statistical). Results of full model not as good as the one obtained above (max) with 2 way interaction of all variables.
              (3B) Correlation between output and feature engineered varaibles is more compared to with just original variables
              (3C) Scatter plots show fan shaped plots (could this be a problem?) 
              (3D) Need to do -- Draw Scatterplots of output variable vs. PCAs [Max for 2 way original]
              (4) Try 3 way interaction of x terms (Nikhil) 
              (5) Full 2 way interactins with domain specific features + LASSO/LARS (no forward/backwatd/stepwise) [Nikhil - include scatterplots of y3 with PCA]
 - [Option 2] Perform LDA/QDA/Bayesian to find grouping [Joanna]
              (1) Rerun with mutually exclusive train test sets [Joanna]
              (2) Run Logistic regression on the data instead of LDA/QDA (NEW) [Joanna]

Nikhil to add outline of paper/presentation (before Sat)
Joanna to start filling details (before Sat)
Meet Sat/Sun to finalize paper/presentation 
 
Week 3 (25th - 31st)
 - [Option 1] PCR with Advanced Feature Engineering [Nikhil]
 - [Option 2] Train LM model with grouping variable (+ interactions) [Joanna] (Future Option -- pipeline of models with Logistic feeding linear regresson)
 
Week 4 (1st - 7th)
 - Prepare Paper and Presentation [All]

PCR with intelligent feature engineering (Option 1)
 
High Cook's D clustering (Option 2)
 - We dont know ahead of time which points belong to high Cook's D unless
 - We could label the points as High Cook's D (HCD) or Low Cook's D (LCD) after running 
 the linear regression
 - Perform LDA/QDA/Logistic Regression to train model to predict the right group for a point
 - Add predicted group as feature (factor) + its interaction term (with what variables?)
 for LM model for predicting y3
 
 OR Train LM model for each group separately to predict y3 
 (but we only have 288 points for HCD group, could be increased by lowering HCD threshold)
 
 Prediction
 - incoming data point >> pass through LDA/QDA/Logistic Regression Model to predict the group
 - Use the LM model for the predicted group to predict the output variable y3
